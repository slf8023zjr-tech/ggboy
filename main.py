
import os
import json
from typing import List, Dict, Any, Union
from langchain_community.chat_models.tongyi import ChatTongyi
from langchain_core.tools import BaseTool
# from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, BaseMessage
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, END

from agent_state import AgentState
from tool.tools import ALL_TOOLS

MAX_HISTORY = 4   # åªä¿ç•™æœ€è¿‘ 4 æ¡ï¼ˆæˆ–ä½ å–œæ¬¢çš„æ•°é‡ï¼‰
PPIO_API_KEY="sk_"
llm = ChatTongyi(
    model="qwen-max", 
    temperature=0,
    api_key="sk-",  
    
)

# ç»‘å®šå·¥å…·åˆ° LLM
llm_with_tools = llm.bind_tools(ALL_TOOLS)

# --- 2. Prompt Template ---
SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªåä¸º OpenManus çš„é«˜çº§ AI ä»£ç†ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·å®Œæˆå¤æ‚çš„ä»»åŠ¡ï¼Œä¸¥æ ¼éµå®ˆæ€è€ƒæ­¥éª¤ã€‚
ä½ çš„å·¥ä½œæµç¨‹éµå¾ª React (Reasoning and Acting) å¾ªç¯ã€‚
ä½ æ‹¥æœ‰ä¸‰å±‚å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼š
1.  **ç¬¬1å±‚ (åŸå­åŒ–)**: ç›´æ¥è°ƒç”¨ `file_read`, `file_write`, `shell_exec`, `search_info`, `plan_task`ã€‚
2.  **ç¬¬2å±‚ (æ²™ç®±å·¥å…·)**: é€šè¿‡è°ƒç”¨ `shell_exec` æ¥æ‰§è¡Œé¢„è£…çš„å‘½ä»¤è¡Œå·¥å…·ï¼ˆä¾‹å¦‚ï¼š`manus-md-to-pdf`, `manus-speech-to-text`ï¼‰ã€‚
3.  **ç¬¬3å±‚ (ä»£ç åŒ…ä¸ API)**: é€šè¿‡è°ƒç”¨ `code_exec` æ¥æ‰§è¡Œ Python ä»£ç ï¼Œç”¨äºå¤æ‚è®¡ç®—ã€æ•°æ®å¤„ç†æˆ– API è°ƒç”¨ã€‚

**ä½ çš„æ€è€ƒæ­¥éª¤ (Thought)**:
1.  **åˆ†æç”¨æˆ·è¯·æ±‚**ï¼šç¡®å®šä»»åŠ¡ç›®æ ‡ã€‚
2.  **é€‰æ‹©å·¥å…·**ï¼šæ ¹æ®ä»»åŠ¡é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·ã€‚
3.  **åˆ¶å®šè®¡åˆ’**ï¼šå¦‚æœä»»åŠ¡å¤æ‚ï¼Œéœ€è¦åˆ†è§£æ­¥éª¤ã€‚
4.  **å†³å®šè¡ŒåŠ¨**ï¼šç”Ÿæˆå·¥å…·è°ƒç”¨ï¼ˆFunction Call JSONï¼‰æˆ–ç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚

**æœ€ç»ˆç­”æ¡ˆ (Final Answer)**:
å½“ä½ è®¤ä¸ºä»»åŠ¡å·²å®Œæˆï¼Œæˆ–è€…æ— æ³•ç»§ç»­æ—¶ï¼Œè¯·ç›´æ¥ç»™å‡ºæœ€ç»ˆç­”æ¡ˆï¼Œä¸è¦å†è¿›è¡Œå·¥å…·è°ƒç”¨ã€‚

**å½“å‰çŠ¶æ€**:
- å†å²å¯¹è¯è®°å½•: {chat_history}
- ä¸Šæ¬¡å·¥å…·æ‰§è¡Œç»“æœ: {last_tool_result}
"""

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", SYSTEM_PROMPT),
        ("placeholder", "{chat_history}"),
        ("human", "{input}"),
    ]
)

# --- 3. Graph Nodes ---



# import json
# from langchain_core.messages import AIMessage

# def planner_node(state: AgentState) -> AgentState:
#     goal = state["input"]
#     context = "æ ¹æ®ç”¨æˆ·ç›®æ ‡ï¼Œåˆ¶å®šè¯¦ç»†è®¡åˆ’"

#     last_tool_result = state.get("last_tool_result", "")

#     formatted = prompt.format(
#         input=goal,
#         goal=goal,
#         context=context,
#         last_tool_result=last_tool_result,
#     )

#     response = llm.invoke(formatted)

#     content = response.content
#     try:
#         if isinstance(content, str):
#             text = content.strip()
#         else:
#             text = "".join(
#                 part["text"] if isinstance(part, dict) and "text" in part else str(part)
#                 for part in content  # type: ignore
#             ).strip()

#         if text.startswith("```"):
#             text = text.strip("`")
#             first_brace = text.find("{")
#             last_brace = text.rfind("}")
#             if first_brace != -1 and last_brace != -1:
#                 text = text[first_brace : last_brace + 1]

#         plan_obj = json.loads(text)

#     except Exception as e:
#         plan_obj = {
#             "goal": goal,
#             "steps": [
#                 {
#                     "index": 1,
#                     "title": "æ— æ³•è§£æ JSONï¼Œè¿”å›åŸå§‹å†…å®¹",
#                     "description": f"åŸå§‹ LLM è¾“å‡ºä¸ºï¼š{content}",
#                     "expected_output": "è¯·ä¸Šå±‚ Agent é‡æ–°è¯·æ±‚è§„åˆ’æˆ–æç¤ºæ¨¡å‹ä¿®æ­£æ ¼å¼ã€‚"
#                 }
#             ],
#             "error": f"JSON parse error: {type(e).__name__}: {e}",
#         }

#     print("è®¡åˆ’ä»»åŠ¡ç»“æœ:")
#     print(json.dumps(plan_obj, indent=2, ensure_ascii=False))

#     # â­ å…³é”®ä¿®æ”¹ï¼šæŠŠ plan å­˜æˆä¸€æ¡ AIMessage çš„åˆ—è¡¨
#     plan_msg = AIMessage(
#         content=json.dumps(plan_obj, ensure_ascii=False)
#     )

#     new_state: AgentState = {
#         **state,
#         "raw_response": response,
#         "plan": [plan_msg],          # ğŸ‘ˆ è¿™é‡Œå˜æˆ list[BaseMessage]
#         "last_tool_result": last_tool_result,
#     }
#     return new_state



def call_llm(state: AgentState) -> Dict[str, Any]:
    """
    è°ƒç”¨ LLM è¿›è¡Œæ¨ç†ï¼Œç”Ÿæˆä¸‹ä¸€æ­¥çš„æ€è€ƒã€å·¥å…·è°ƒç”¨æˆ–æœ€ç»ˆç­”æ¡ˆã€‚
    """
    print("--- Node: call_llm ---")
    
    # å‡†å¤‡è¾“å…¥æ¶ˆæ¯
    messages = state["chat_history"] + [HumanMessage(content=state["input"])]
    
    # å¦‚æœæ˜¯å·¥å…·æ‰§è¡Œåçš„è¿”å›ï¼Œéœ€è¦å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²ä¸­
    if state.get("last_tool_result"):
        # æ‰¾åˆ°ä¸Šä¸€ä¸ª AIMessage (åŒ…å«å·¥å…·è°ƒç”¨çš„é‚£ä¸ª)
        last_ai_message = next((msg for msg in reversed(messages) if isinstance(msg, AIMessage) and msg.tool_calls), None)
        
        if last_ai_message:
            # åˆ›å»º ToolMessage
            tool_messages = []
            for i, tool_call in enumerate(last_ai_message.tool_calls):
                # å‡è®¾æˆ‘ä»¬åªå¤„ç†ä¸Šä¸€ä¸ªå·¥å…·è°ƒç”¨çš„ç»“æœ
                tool_messages.append(ToolMessage(
                    content=state["last_tool_result"],
                    tool_call_id=tool_call["id"],
                ))
            
            # å°†å·¥å…·æ¶ˆæ¯æ·»åŠ åˆ°å†å²ä¸­ï¼Œç„¶åæ˜¯ç”¨æˆ·è¾“å…¥
            messages = state["chat_history"] + tool_messages + [HumanMessage(content=state["input"])]
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å·¥å…·è°ƒç”¨ï¼Œè¯´æ˜æ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œæˆ–çŠ¶æ€å¼‚å¸¸ï¼Œç›´æ¥ç”¨ç”¨æˆ·è¾“å…¥
            messages = state["chat_history"] + [HumanMessage(content=state["input"])]

    # æ ¼å¼åŒ–ç³»ç»Ÿæç¤ºè¯
    formatted_prompt = prompt.format(
        chat_history=messages,
        input=state["input"],
        last_tool_result=state.get("last_tool_result", "æ— ")
    )
    
    # è°ƒç”¨ LLM
    response = llm_with_tools.invoke(formatted_prompt)
    # print(f"LLM Raw Response:\n{response}")
    
    # æ›´æ–°çŠ¶æ€
    new_messages = state["chat_history"] + [response] # <-- Added plan to chat history
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ç»ˆç­”æ¡ˆ
    # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œå³ä½¿ content ä¸ºç©ºä¹Ÿåº”è¯¥ä½œä¸ºæœ€ç»ˆç­”æ¡ˆ
    final_answer = None
    if not response.tool_calls:
        # å³ä½¿ content ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œä¹Ÿè®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªç­”æ¡ˆï¼ˆé¿å…é™·å…¥å¾ªç¯ï¼‰
        final_answer = response.content if response.content else "[LLMè¿”å›ç©ºå“åº”]"
    if len(new_messages) > MAX_HISTORY:
        new_messages = new_messages[-MAX_HISTORY:]
    return {
        "chat_history": new_messages,
        "agent_outcome": response,
        "final_answer": final_answer,
        "last_tool_result": None, # æ¸…ç©ºä¸Šæ¬¡å·¥å…·ç»“æœ
        "iteration": state.get("iteration", 0) + 1
    }

def call_tool(state: AgentState) -> Dict[str, Any]:
    """
    æ‰§è¡Œ LLM å»ºè®®çš„å·¥å…·è°ƒç”¨ã€‚
    """
    print("--- Node: call_tool ---")
    
    agent_outcome = state["agent_outcome"]
    tool_calls = agent_outcome.tool_calls
    
    if not tool_calls:
        # ç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºè¾¹å·²ç»å¤„ç†äº†è¿™ç§æƒ…å†µ
        return {"last_tool_result": "Error: call_tool node reached without tool calls."}

    # å‡è®¾æˆ‘ä»¬åªå¤„ç†ç¬¬ä¸€ä¸ªå·¥å…·è°ƒç”¨
    tool_call = tool_calls[0]
    tool_name = tool_call["name"]
    tool_args = tool_call["args"]
    plan = tool_call.get("plan", None)
    
    print(f"Executing Tool: {tool_name} with args: {tool_args}")
    
    # æŸ¥æ‰¾å¹¶æ‰§è¡Œå·¥å…·
    tool_func = next((t for t in ALL_TOOLS if t.name == tool_name), None)
    
    if not tool_func:
        result = f"Error: Tool '{tool_name}' not found."
    else:
        try:
            # æ‰§è¡Œå·¥å…·å‡½æ•°
            result = tool_func.invoke(tool_args)
            if tool_name == "plan_task":
                # å¦‚æœæ˜¯ plan_task å·¥å…·ï¼Œæ ¼å¼åŒ–è¾“å‡ºä¸ºå¯è¯»æ–‡æœ¬
                plan = json.loads(result)
                print("è®¡åˆ’ä»»åŠ¡ç»“æœ:")
                print(json.dumps(json.loads(result), indent=2, ensure_ascii=False))
        except Exception as e:
            result = f"Tool Execution Error in '{tool_name}': {type(e).__name__}: {e}"
    result_str = str(result)
    print(f"Tool Result: {result_str[:100]}...")
    
    # æ›´æ–°çŠ¶æ€
    return {
        "plan": plan,
        "last_tool_name": tool_name,
        "last_tool_result": result,
        "input": state["input"], # ä¿æŒç”¨æˆ·è¾“å…¥ä¸å˜ï¼Œä»¥ä¾¿ LLM çŸ¥é“è¦ç»§ç»­è§£å†³å“ªä¸ªé—®é¢˜
        "chat_history": state["chat_history"] # å†å²æ¶ˆæ¯å·²åœ¨ call_llm ä¸­æ›´æ–°
    }

# --- 4. Graph Edges (Conditional Logic) ---

def should_continue(state: AgentState) -> str:
    """
    æ ¹æ® LLM çš„è¾“å‡ºå†³å®šä¸‹ä¸€æ­¥æ˜¯ç»§ç»­è°ƒç”¨å·¥å…·è¿˜æ˜¯ç»“æŸã€‚
    """
    print("--- Edge: should_continue ---")
    
    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰
    MAX_ITERATIONS = 10
    if state.get("iteration", 0) >= MAX_ITERATIONS:
        print(f"Max iterations ({MAX_ITERATIONS}) reached. Ending.")
        return "end"
    
    # æ£€æŸ¥ LLM æ˜¯å¦å»ºè®®å·¥å…·è°ƒç”¨ï¼ˆä¼˜å…ˆæ£€æŸ¥ï¼‰
    agent_outcome = state.get("agent_outcome")
    # print(" agent_outcome", agent_outcome)
    if isinstance(agent_outcome, BaseMessage):
        tool_calls = getattr(agent_outcome, 'tool_calls', None)
        if tool_calls:
            print(f"âœ… Tool call suggested: {tool_calls[0]['name']}. Continuing to call_tool.")
            return "continue"
    
    # æ£€æŸ¥ LLM æ˜¯å¦ç»™å‡ºäº†æœ€ç»ˆç­”æ¡ˆ
    final_answer = state.get("final_answer")
    if final_answer:
        print(f"âœ… Final answer found: {final_answer[:100]}...")
        return "end"
    
    # é»˜è®¤æƒ…å†µä¸‹ç»“æŸ
    print("âŒ No tool call and no final answer. Ending.")
    print(f"   agent_outcome: {agent_outcome}")
    print(f"   final_answer: {final_answer}")
    return "end"

# --- 5. Build the Graph ---

def build_graph():
    
    
    # create_react_agent()
    workflow = StateGraph(AgentState)

    # 1. å®šä¹‰èŠ‚ç‚¹
    workflow.add_node("llm", call_llm)
    workflow.add_node("tool", call_tool)
    # workflow.add_node("planner", planner_node)

    # 2. è®¾ç½®å…¥å£
    workflow.set_entry_point("llm")
    
    # workflow.add_edge("planner", "llm")

    # 3. å®šä¹‰è¾¹
    # ä» LLM èŠ‚ç‚¹å‡ºå‘ï¼Œæ ¹æ® should_continue çš„ç»“æœå†³å®šä¸‹ä¸€æ­¥
    workflow.add_conditional_edges(
        "llm",
        should_continue,
        {
            "continue": "tool", # å¦‚æœéœ€è¦å·¥å…·ï¼Œè½¬åˆ° tool èŠ‚ç‚¹
            "end": END           # å¦‚æœæ˜¯æœ€ç»ˆç­”æ¡ˆæˆ–è¾¾åˆ°é™åˆ¶ï¼Œç»“æŸ
        }
    )

    # ä» Tool èŠ‚ç‚¹å‡ºå‘ï¼Œæ‰§è¡Œå®Œå·¥å…·åï¼Œæ€»æ˜¯è¿”å› LLM è¿›è¡Œä¸‹ä¸€æ­¥æ¨ç†
    workflow.add_edge("tool", "llm")

    # 4. ç¼–è¯‘å›¾
    app = workflow.compile()
    return app

# --- 6. Main Execution ---

if __name__ == "__main__":
    from langchain_core.messages import HumanMessage, AIMessage, BaseMessage

    # ç¼–è¯‘ Agent
    app = build_graph()
    print("--- OpenManus LangGraph Agent Initialized ---")
    print("ç°åœ¨å¯ä»¥ç›´æ¥å’Œ Agent å¯¹è¯äº†ï¼Œè¾“å…¥ exit/quit ç»“æŸä¼šè¯ã€‚\n")

    # æŒä¹…åŒ–å¯¹è¯å†å² & è¿­ä»£è®¡æ•°
    chat_history: list[BaseMessage] = []
    iteration = 0

    while True:
        try:
            user_input = input("ä½ ï¼š").strip()
        except (KeyboardInterrupt, EOFError):
            print("\n[ç³»ç»Ÿ] ä¼šè¯ç»“æŸï¼Œå†è§ï½")
            break

        if not user_input:
            continue

        if user_input.lower() in {"exit", "quit", "q"}:
            print("[ç³»ç»Ÿ] å·²é€€å‡ºå¯¹è¯ã€‚")
            break

        # æ„é€ çŠ¶æ€ï¼ˆè¿™ä¸€éƒ¨åˆ†å­—æ®µå¿…é¡»å’Œ AgentState å¯¹é½ï¼‰
        state = {
            "input": user_input,
            "chat_history": chat_history,
            "final_answer": None,
            "last_tool_name": None,
            "last_tool_result": None,
            "iteration": iteration,
        }

        # ğŸ‘‡ æ–¹å¼ä¸€ï¼šä¸€æ­¥åˆ°ä½æ‹¿æœ€ç»ˆç»“æœï¼ˆæ¨èæ—¥å¸¸ä½¿ç”¨ï¼‰
        result_state = app.invoke(state)

        # å¦‚æœä½ æ›´æƒ³çœ‹ä¸­é—´ ReAct è¿‡ç¨‹ï¼Œå¯ä»¥æ”¹ç”¨ streamï¼š
        # result_state = None
        # for step in app.stream(state):
        #     # step æ˜¯ä¸€ä¸ª {node_name: partial_state} çš„å¢é‡
        #     result_state = list(step.values())[-1]
        #     # æƒ³è°ƒè¯•çš„è¯ï¼Œè¿™é‡Œå¯ä»¥æ‰“å°æ¯ä¸€æ­¥ï¼š
        #     # print(step, "\n---")
        #
        # assert result_state is not None

        answer = result_state.get("final_answer") or "ï¼ˆAgent æ²¡æœ‰è¿”å› final_answer å­—æ®µâ€¦â€¦ï¼‰"
        print(f"Agentï¼š{answer}\n")

        # æ›´æ–°å¯¹è¯å†å²ï¼Œä¾›ä¸‹ä¸€è½®ä½¿ç”¨
        chat_history.append(HumanMessage(content=user_input))
        chat_history.append(AIMessage(content=answer))

        # åŒæ­¥è¿­ä»£è®¡æ•°ï¼ˆå¦‚æœå›¾é‡Œæœ‰æ›´æ–°çš„è¯ï¼‰
        iteration = result_state.get("iteration", iteration + 1)

