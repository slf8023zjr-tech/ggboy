import os
import json
from typing import List, Dict, Any

from langchain_community.llms import Tongyi
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langgraph.graph import StateGraph, END

from agent_state import AgentState
from tools import ALL_TOOLS
from config import AGENT_MODEL, MAX_ITERATIONS, DASHSCOPE_API_KEY

# --- 1. LLM Setup ---
# è®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡
if DASHSCOPE_API_KEY:
    os.environ["DASHSCOPE_API_KEY"] = DASHSCOPE_API_KEY

# ä½¿ç”¨ Tongyi æ¨¡å‹
# æ³¨æ„ï¼šTongyi æ¨¡å‹æ˜¯ LLM ç±»å‹ï¼Œä¸æ”¯æŒ OpenAI æ ¼å¼çš„ Function Callingã€‚
# æˆ‘ä»¬å°†é€šè¿‡ prompt æŒ‡å¯¼å®ƒè¾“å‡ºç‰¹å®šæ ¼å¼çš„ JSON æ¥æ¨¡æ‹Ÿå·¥å…·è°ƒç”¨ã€‚
llm = Tongyi(
    model=AGENT_MODEL, 
    temperature=0,
)

# --- 2. Prompt Template ---
SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªåä¸º OpenManus çš„é«˜çº§ AI ä»£ç†ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·å®Œæˆå¤æ‚çš„ä»»åŠ¡ã€‚
ä½ çš„å·¥ä½œæµç¨‹éµå¾ª React (Reasoning and Acting) å¾ªç¯ã€‚
ä½ æ‹¥æœ‰ä¸‰å±‚å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼š
1.  **ç¬¬1å±‚ (åŸå­åŒ–)**: ç›´æ¥è°ƒç”¨ `file_read`, `file_write`, `shell_exec`, `search_info`ã€‚
2.  **ç¬¬2å±‚ (æ²™ç®±å·¥å…·)**: é€šè¿‡è°ƒç”¨ `shell_exec` æ¥æ‰§è¡Œé¢„è£…çš„å‘½ä»¤è¡Œå·¥å…·ï¼ˆä¾‹å¦‚ï¼š`manus-md-to-pdf`ï¼‰ã€‚
3.  **ç¬¬3å±‚ (ä»£ç åŒ…ä¸ API)**: é€šè¿‡è°ƒç”¨ `code_exec` æ¥æ‰§è¡Œ Python ä»£ç ï¼Œç”¨äºå¤æ‚è®¡ç®—ã€æ•°æ®å¤„ç†æˆ– API è°ƒç”¨ã€‚

**ç”±äºå½“å‰æ¨¡å‹ä¸æ”¯æŒæ ‡å‡†çš„ Function Calling JSON æ ¼å¼ï¼Œä½ éœ€è¦ä»¥ç‰¹å®šçš„ Markdown æ ¼å¼è¾“å‡ºä½ çš„è¡ŒåŠ¨ã€‚**

**è¡ŒåŠ¨æ ¼å¼**:
å¦‚æœä½ éœ€è¦ä½¿ç”¨å·¥å…·ï¼Œè¯·ä»¥ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œä¸”ä»…è¾“å‡ºä¸€ä¸ª action å—ï¼š
```action
{{"tool_name": "å·¥å…·åç§°", "tool_args": {{"å‚æ•°1": "å€¼1", "å‚æ•°2": "å€¼2"}}}}
```
**æ³¨æ„**: `tool_name` å¿…é¡»æ˜¯ `file_read`, `file_write`, `shell_exec`, `search_info`, æˆ– `code_exec` ä¹‹ä¸€ã€‚

**ä½ çš„æ€è€ƒæ­¥éª¤ (Thought)**:
1.  **åˆ†æç”¨æˆ·è¯·æ±‚**ï¼šç¡®å®šä»»åŠ¡ç›®æ ‡ã€‚
2.  **é€‰æ‹©å·¥å…·**ï¼šæ ¹æ®ä»»åŠ¡é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·ã€‚
3.  **åˆ¶å®šè®¡åˆ’**ï¼šå¦‚æœä»»åŠ¡å¤æ‚ï¼Œéœ€è¦åˆ†è§£æ­¥éª¤ã€‚
4.  **å†³å®šè¡ŒåŠ¨**ï¼šç”Ÿæˆä¸Šè¿° `action` æ ¼å¼çš„ Markdown ä»£ç å—ï¼Œæˆ–ç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚

**æœ€ç»ˆç­”æ¡ˆ (Final Answer)**:
å½“ä½ è®¤ä¸ºä»»åŠ¡å·²å®Œæˆï¼Œæˆ–è€…æ— æ³•ç»§ç»­æ—¶ï¼Œè¯·ç›´æ¥ç»™å‡ºæœ€ç»ˆç­”æ¡ˆï¼Œä¸è¦å†è¿›è¡Œå·¥å…·è°ƒç”¨ã€‚

**å½“å‰çŠ¶æ€**:
- å†å²å¯¹è¯è®°å½•:
{CHAT_HISTORY}
- ä¸Šæ¬¡å·¥å…·æ‰§è¡Œç»“æœ: {LAST_TOOL_RESULT}
"""

# --- 3. Graph Nodes ---

def call_llm(state: AgentState) -> Dict[str, Any]:
    """
    è°ƒç”¨ LLM è¿›è¡Œæ¨ç†ï¼Œç”Ÿæˆä¸‹ä¸€æ­¥çš„æ€è€ƒã€å·¥å…·è°ƒç”¨æˆ–æœ€ç»ˆç­”æ¡ˆã€‚
    """
    print("--- Node: call_llm ---")
    
    # 1. æ„å»ºå†å²æ¶ˆæ¯å­—ç¬¦ä¸²
    history_str = ""
    for msg in state["chat_history"]:
        if isinstance(msg, HumanMessage):
            history_str += f"ç”¨æˆ·: {msg.content}\n"
        elif isinstance(msg, AIMessage):
            history_str += f"AI: {msg.content}\n"
    
    if state.get("last_tool_result"):
        history_str += f"å·¥å…· {state['last_tool_name']} æ‰§è¡Œç»“æœ: {state['last_tool_result']}\n"

    # 2. ç»„åˆæœ€ç»ˆè¾“å…¥ï¼ˆä½¿ç”¨å¤§å†™å ä½ç¬¦é¿å… JSON ä¸­çš„å¤§æ‹¬å·å†²çªï¼‰
    final_input = SYSTEM_PROMPT.format(
        CHAT_HISTORY=history_str,
        LAST_TOOL_RESULT=state.get("last_tool_result", "æ— ")
    ) + f"\nç”¨æˆ·è¾“å…¥: {state['input']}"
    
    # è°ƒç”¨ LLM
    response = llm.invoke(final_input)
    print(f"LLM Raw Response:\n{response}")

    # --- æ‰‹åŠ¨è§£æ Tongyi çš„è¾“å‡º ---
    tool_calls = []
    final_answer = None
    content = response

    if "```action" in response:
        try:
            action_start = response.find("```action") + len("```action")
            action_end = response.find("```", action_start)
            action_json_str = response[action_start:action_end].strip()
            action_data = json.loads(action_json_str)
            
            tool_calls.append({
                "name": action_data["tool_name"],
                "args": action_data["tool_args"],
                "id": f"call_{os.urandom(8).hex()}"
            })
            content = response[:response.find("```action")]
        except Exception as e:
            print(f"Warning: Failed to parse action JSON: {e}")

    if not tool_calls:
        final_answer = content
        
    ai_message = AIMessage(content=content, tool_calls=tool_calls)
    
    return {
        "chat_history": state["chat_history"] + [ai_message],
        "agent_outcome": ai_message,
        "final_answer": final_answer,
        "last_tool_result": None,
        "iteration": state.get("iteration", 0) + 1
    }

def call_tool(state: AgentState) -> Dict[str, Any]:
    """
    æ‰§è¡Œ LLM å»ºè®®çš„å·¥å…·è°ƒç”¨ã€‚
    """
    print("--- Node: call_tool ---")
    agent_outcome = state["agent_outcome"]
    tool_calls = agent_outcome.tool_calls
    
    if not tool_calls:
        return {"last_tool_result": "Error: call_tool node reached without tool calls."}

    tool_call = tool_calls[0]
    tool_name = tool_call["name"]
    tool_args = tool_call["args"]
    
    print(f"Executing Tool: {tool_name} with args: {tool_args}")
    
    tool_func = next((t for t in ALL_TOOLS if t.name == tool_name), None)
    
    if not tool_func:
        result = f"Error: Tool '{tool_name}' not found."
    else:
        try:
            result = tool_func.invoke(tool_args)
        except Exception as e:
            result = f"Tool Execution Error in '{tool_name}': {type(e).__name__}: {e}"
            
    print(f"Tool Result: {result[:100]}...")
    
    return {
        "last_tool_name": tool_name,
        "last_tool_result": result,
    }

# --- 4. Graph Edges (Conditional Logic) ---
def should_continue(state: AgentState) -> str:
    print("--- Edge: should_continue ---")
    if state.get("iteration", 0) >= MAX_ITERATIONS:
        print(f"Max iterations ({MAX_ITERATIONS}) reached. Ending.")
        return "end"
    if state.get("final_answer"):
        print("Final answer found. Ending.")
        return "end"
    agent_outcome = state.get("agent_outcome")
    if isinstance(agent_outcome, BaseMessage) and agent_outcome.tool_calls:
        print(f"Tool call suggested: {agent_outcome.tool_calls[0]['name']}. Continuing to call_tool.")
        return "continue"
    print("No tool call and no final answer. Ending.")
    return "end"

# --- 5. Build the Graph ---
def build_graph():
    workflow = StateGraph(AgentState)
    workflow.add_node("llm", call_llm)
    workflow.add_node("tool", call_tool)
    workflow.set_entry_point("llm")
    workflow.add_conditional_edges(
        "llm",
        should_continue,
        {"continue": "tool", "end": END}
    )
    workflow.add_edge("tool", "llm")
    return workflow.compile()

# --- 6. Main Execution ---
if __name__ == "__main__":
    app = build_graph()
    print("--- OpenManus LangGraph Agent Initialized (Using Tongyi) ---")
    
    # äº¤äº’å¼å¾ªç¯ - æ¥æ”¶ç”¨æˆ·è¾“å…¥
    print("\n" + "="*70)
    print("ğŸ¤– OpenManus Agent äº¤äº’æ¨¡å¼ï¼ˆTongyi æ¨¡å‹ + å·¥å…·è°ƒç”¨ï¼‰")
    print("="*70)
    print("è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡ºç¨‹åº\n")
    
    chat_history = []
    
    while True:
        # è·å–ç”¨æˆ·è¾“å…¥
        user_input = input("\nğŸ‘¤ ä½ : ").strip()
        
        if not user_input:
            continue
        
        if user_input.lower() in ['exit', 'quit', 'é€€å‡º']:
            print("ğŸ‘‹ å†è§ï¼")
            break
        
        # åˆ›å»ºåˆå§‹çŠ¶æ€
        initial_state = {
            "input": user_input,
            "chat_history": chat_history,
            "final_answer": None,
            "last_tool_name": None,
            "last_tool_result": None,
            "iteration": 0,
            "agent_outcome": None,
        }
        
        print("\n" + "-"*70)
        print("ğŸ”„ Agent å¤„ç†ä¸­...\n")
        
        try:
            # è¿è¡Œ Agent
            for s in app.stream(initial_state):
                pass  # è®©åº•å±‚å¤„ç†æµç¨‹è¾“å‡ºè°ƒè¯•ä¿¡æ¯
            
            # è·å–æœ€ç»ˆçŠ¶æ€
            result = app.invoke(initial_state)
            
            # æ›´æ–°èŠå¤©å†å²
            chat_history = result.get("chat_history", chat_history)
            
            # æ˜¾ç¤ºç»“æœ
            print("\n" + "-"*70)
            print(f"ğŸ¤– ä»£ç†å›å¤: {result.get('final_answer', 'æ— æ³•ç”Ÿæˆç­”æ¡ˆ')}")
            
            if result.get('last_tool_result'):
                print(f"\nğŸ”§ å·¥å…·æ‰§è¡Œç»“æœ:\n{result['last_tool_result']}")
            
            print(f"\nâ±ï¸  è¿­ä»£æ¬¡æ•°: {result.get('iteration', 0)}")
            print("-"*70)
        
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {type(e).__name__}: {e}")
            print("-"*70)
            continue