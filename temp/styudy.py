from langchain_community.llms import Tongyi
from langgraph.graph import StateGraph, END
from typing import TypedDict, Annotated, Optional, List, Dict, Any
from operator import add
import os
from tools import ALL_TOOLS
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, BaseMessage

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["DASHSCOPE_API_KEY"] = 'sk-7105444cad4d4699806f10612e4c9a25'

# ä½¿ç”¨ TypedDict å’Œ Annotated å®šä¹‰çŠ¶æ€ schema
class AgentState(TypedDict):
    input: str
    chat_history: Annotated[List[BaseMessage], add]
    final_answer: Annotated[Optional[str], lambda x, y: y]
    last_tool_name: Annotated[Optional[str], lambda x, y: y]
    last_tool_result: Annotated[Optional[str], lambda x, y: y]
    iteration: Annotated[int, lambda x, y: y]
    agent_outcome: Annotated[Optional[Any], lambda x, y: y]  # LLM çš„è¾“å‡º

model = Tongyi()

# ========== LLM èŠ‚ç‚¹ ==========
def call_llm(state: AgentState) -> Dict[str, Any]:
    """è°ƒç”¨ LLM è¿›è¡Œæ¨ç†ï¼Œç”Ÿæˆä¸‹ä¸€æ­¥çš„æ€è€ƒã€å·¥å…·è°ƒç”¨æˆ–æœ€ç»ˆç­”æ¡ˆ"""
    print("\n--- Node: call_llm ---")
    
    # è·å–ç”¨æˆ·è¾“å…¥
    user_input = state["input"]
    
    # è°ƒç”¨æ¨¡å‹
    print(f"ğŸ“ å¤„ç†è¾“å…¥: {user_input}")
    response = model.invoke(user_input)
    
    print(f"ğŸ¤– LLM å“åº”: {response[:100]}..." if len(response) > 100 else f"ğŸ¤– LLM å“åº”: {response}")
    
    # æ„é€ æ–°çš„èŠå¤©å†å²
    new_messages = state["chat_history"] + [
        HumanMessage(content=user_input),
        AIMessage(content=response)
    ]
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨æ ‡è®°ï¼ˆç®€å•å¯å‘å¼æ–¹æ³•ï¼‰
    # åœ¨å®é™…åº”ç”¨ä¸­ï¼ŒLLM åº”è¯¥è¿”å›ç»“æ„åŒ–çš„å·¥å…·è°ƒç”¨æ ¼å¼
    has_tool_call = False
    tool_calls = []
    
    # æ›´æ–°çŠ¶æ€
    return {
        "chat_history": new_messages,
        "agent_outcome": response,
        "final_answer": None if has_tool_call else response,
        "iteration": state["iteration"] + 1
    }

# ========== å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹ ==========
def call_tool(state: AgentState) -> Dict[str, Any]:
    """æ‰§è¡Œ LLM å»ºè®®çš„å·¥å…·è°ƒç”¨"""
    print("\n--- Node: call_tool ---")
    
    agent_outcome = state["agent_outcome"]
    
    # ç®€å•æ¼”ç¤ºï¼šè§£æå“åº”ä¸­çš„å·¥å…·åç§°
    # åœ¨å®é™…åº”ç”¨ä¸­åº”è¯¥æœ‰æ›´å®Œå–„çš„å·¥å…·è°ƒç”¨æ ¼å¼
    tools_names = [tool.name for tool in ALL_TOOLS]
    
    tool_name = None
    tool_args = {}
    
    # æ£€æŸ¥å“åº”ä¸­æ˜¯å¦åŒ…å«å·¥å…·åç§°
    for tool in tools_names:
        if tool in agent_outcome.lower():
            tool_name = tool
            break
    
    if not tool_name:
        print("âš ï¸  æœªæ£€æµ‹åˆ°å·¥å…·è°ƒç”¨")
        return {
            "last_tool_name": None,
            "last_tool_result": "æœªæ£€æµ‹åˆ°å·¥å…·è°ƒç”¨"
        }
    
    print(f"ğŸ”§ æ‰§è¡Œå·¥å…·: {tool_name}")
    
    # æŸ¥æ‰¾å·¥å…·
    tool_func = next((t for t in ALL_TOOLS if t.name == tool_name), None)
    
    if not tool_func:
        result = f"âŒ é”™è¯¯: å·¥å…· '{tool_name}' ä¸å­˜åœ¨"
    else:
        try:
            # æ ¹æ®å·¥å…·ç±»å‹æ„é€ å‚æ•°
            if tool_name == "file_read":
                result = tool_func.invoke({"path": "./styudy.py"})
            elif tool_name == "code_exec":
                result = tool_func.invoke({"code": "print('Hello from code_exec!')"})
            elif tool_name == "search_info":
                result = tool_func.invoke({"queries": ["Tongyi LLM", "LangGraph"]})
            else:
                result = tool_func.invoke({})
        except Exception as e:
            result = f"âŒ å·¥å…·æ‰§è¡Œé”™è¯¯: {type(e).__name__}: {e}"
    
    print(f"âœ… å·¥å…·ç»“æœ: {result[:100]}..." if len(result) > 100 else f"âœ… å·¥å…·ç»“æœ: {result}")
    
    return {
        "last_tool_name": tool_name,
        "last_tool_result": result,
        "iteration": state["iteration"] + 1,
        "chat_history": state["chat_history"] + [ToolMessage(content=result, tool_call_id="")]
    }

# ========== æ¡ä»¶åˆ¤æ–­å‡½æ•° ==========
def should_continue(state: AgentState) -> str:
    """æ ¹æ® LLM çš„è¾“å‡ºå†³å®šä¸‹ä¸€æ­¥ï¼šç»§ç»­è°ƒç”¨å·¥å…·è¿˜æ˜¯ç»“æŸ"""
    print("\n--- Edge: should_continue ---")
    
    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
    MAX_ITERATIONS = 5
    if state.get("iteration", 0) >= MAX_ITERATIONS:
        print(f"â¹ï¸  è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° ({MAX_ITERATIONS})")
        return "end"
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰æœ€ç»ˆç­”æ¡ˆ
    if state.get("final_answer"):
        print("âœ… æœ€ç»ˆç­”æ¡ˆå·²ç”Ÿæˆï¼Œç»“æŸ")
        return "end"
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦å·¥å…·
    agent_outcome = state.get("agent_outcome", "")
    tools_names = [tool.name for tool in ALL_TOOLS]
    
    # ç®€å•å¯å‘å¼æ–¹æ³•ï¼šæ£€æŸ¥å“åº”ä¸­æ˜¯å¦åŒ…å«å·¥å…·åç§°
    for tool in tools_names:
        if tool in agent_outcome.lower():
            print(f"ğŸ”§ æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨: {tool}ï¼Œç»§ç»­...")
            return "continue"
    
    print("ğŸ“Œ æœªæ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼Œç»“æŸ")
    return "end"

# ========== æ„å»ºå›¾ ==========
def build_graph():
    """æ„å»º LangGraph å·¥ä½œæµ"""
    workflow = StateGraph(AgentState)

    # 1. å®šä¹‰èŠ‚ç‚¹
    workflow.add_node("llm", call_llm)
    workflow.add_node("tool", call_tool)

    # 2. è®¾ç½®å…¥å£
    workflow.set_entry_point("llm")

    # 3. å®šä¹‰æ¡ä»¶è¾¹
    workflow.add_conditional_edges(
        "llm",
        should_continue,
        {
            "continue": "tool",  # å¦‚æœéœ€è¦å·¥å…·ï¼Œè½¬åˆ° tool èŠ‚ç‚¹
            "end": END           # å¦‚æœæ˜¯æœ€ç»ˆç­”æ¡ˆæˆ–è¾¾åˆ°é™åˆ¶ï¼Œç»“æŸ
        }
    )

    # ä» Tool èŠ‚ç‚¹è¿”å› LLM è¿›è¡Œä¸‹ä¸€æ­¥æ¨ç†
    workflow.add_edge("tool", "llm")

    # 4. ç¼–è¯‘å›¾
    app = workflow.compile()
    return app

# ========== ä¸»ç¨‹åº ==========
if __name__ == "__main__":
    print("=" * 70)
    print("ğŸ¤– OpenManus Agent (Tongyi + LangGraph + å·¥å…·è°ƒç”¨)")
    print("=" * 70)
    
    # æ„å»ºå›¾
    app = build_graph()
    
    # äº¤äº’å¼å¾ªç¯
    chat_history = []
    
    while True:
        print("\n" + "=" * 70)
        user_input = input("ä½ : ").strip()
        
        if not user_input:
            continue
        
        if user_input.lower() in ['exit', 'quit', 'é€€å‡º']:
            print("å†è§ï¼")
            break
        
        # åˆ›å»ºåˆå§‹çŠ¶æ€
        initial_state = {
            "input": user_input,
            "chat_history": chat_history,
            "final_answer": None,
            "last_tool_name": None,
            "last_tool_result": None,
            "iteration": 0,
            "agent_outcome": None
        }
        
        # è¿è¡Œ Agent
        try:
            result = app.invoke(initial_state)
            
            # æ›´æ–°èŠå¤©å†å²
            chat_history = result.get("chat_history", chat_history)
            
            # è¾“å‡ºç»“æœ
            print("\n" + "=" * 70)
            print(f"ğŸ¤– ä»£ç†: {result.get('final_answer', 'æ— æ³•ç”Ÿæˆç­”æ¡ˆ')}")
            if result.get('last_tool_result'):
                print(f"ğŸ”§ æœ€åå·¥å…·ç»“æœ: {result['last_tool_result'][:200]}...")
            print(f"â±ï¸  è¿­ä»£æ¬¡æ•°: {result.get('iteration', 0)}")
            print("=" * 70)
        
        except Exception as e:
            print(f"âŒ é”™è¯¯: {type(e).__name__}: {e}")
            continue
